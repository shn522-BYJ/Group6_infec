# -*- coding: utf-8 -*-
"""parasitic disease.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PayGzPDZyOH3PDfS_xmcEuNYyfFE4l4W
"""


# Load datasets
import pandas as pd
articles_data = pd.read_csv("data/articles.schistosomiasis.csv")
# Added encoding='latin-1' to handle the different file encoding
authors_data = pd.read_csv("data/authors.schistosomiasis.csv")


articles_data

# Check for missing values
print(articles_data.isnull().sum())

from collections import Counter
import re

# Combine titles and abstracts into a single text
combined_text = " ".join(articles_data['Title'].dropna().tolist() +
                         articles_data['Abstract'].dropna().tolist()).lower()

# Tokenize and clean text
words = re.findall(r'\b\w+\b', combined_text)
stopwords = set(['the', 'and', 'of', 'to', 'in', 'for', 'with', 'on', 'a', 'by', 'as', 'an', 'at', 'or', 'from', 'that', 'this','among','after',
          'based','more','than','both','have','these','study','other','between','been','which','were','china','also'])
filtered_words = [word for word in words if word not in stopwords and len(word) > 3]

# Count word frequencies
word_counts = Counter(filtered_words)
top_keywords = word_counts.most_common(50)

# Display the top 20 most frequent words as a pandas DataFrame
top_keywords_df = pd.DataFrame(top_keywords, columns=["Keyword", "Count"])

# Print the DataFrame to the console
print(top_keywords_df)

# Save the results to a CSV file for further review
top_keywords_df.to_csv('data/top_keywords_in_first_round.csv', index=False)

# Define the main keywords related to schistosomiasis
keywords = ['schistosomiasis','parasitic','parasite','japonicum','schistosoma','praziquantel','schistosome','mansoni','haematobium','helminth','trematodes']

# Use a case-insensitive search in the Title and Abstract columns
filtered_articles = articles_data[
    articles_data['Title'].str.contains('|'.join(keywords), case=False) |
    articles_data['Abstract'].str.contains('|'.join(keywords), case=False)
]

# Save or explore the filtered articles
filtered_articles.to_csv('data/filtered_articles_simple.csv', index=False)
print(f"Number of articles mentioning the target: {filtered_articles.shape[0]}")

filtered_articles

authors_data

# Check for missing values
print(authors_data.isnull().sum())

# Filter authors based on selected PMIDs
relevant_authors = authors_data[authors_data['PMID'].isin(filtered_articles['PMID'])]
relevant_authors.to_csv('data/filtered_authors.csv', index=False)

filtered_authors = pd.read_csv("data/filtered_authors.csv")
filtered_authors

# Check for missing values
print(filtered_authors.isnull().sum())

# Replace missing forenames with a placeholder (e.g., 'Unknown')
filtered_authors['AuthorForename'] = filtered_authors['AuthorForename'].fillna('Unknown')
# Check for missing values
print(filtered_authors.isnull().sum())

key_researchers = (
    filtered_authors.groupby(['AuthorForename', 'AuthorLastname'])
    .size()
    .reset_index(name='ContributionCount')
)

key_researchers = key_researchers.sort_values(by='ContributionCount', ascending=False)
key_researchers.to_csv('key_researchers.csv', index=False)
print(key_researchers.head(10))
