{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get relevant articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by removing brackets, special characters, and trimming whitespace.\"\"\"\n",
    "    if pd.isnull(text):  # Handle missing data\n",
    "        return \"\"\n",
    "    # Remove brackets, special characters, and strip whitespace\n",
    "    return re.sub(r'[\\[\\](){}]', '', text).strip().lower()\n",
    "\n",
    "def contains_keywords(text, keywords):\n",
    "    \"\"\"Check if any keyword is present in the given text as a whole word.\"\"\"\n",
    "    text_normalized = normalize_text(text)\n",
    "    # Use regex to ensure keywords are matched as whole words\n",
    "    return any(re.search(rf'\\b{re.escape(keyword)}\\b', text_normalized) for keyword in keywords)\n",
    "\n",
    "def filter_articles(articles_df, keywords, sample_size=5):\n",
    "    \"\"\"\n",
    "    Filter articles based on keywords in titles and abstracts.\n",
    "    Randomly sample non-relevant articles for manual review.\n",
    "\n",
    "    Args:\n",
    "        articles_df (pd.DataFrame): DataFrame containing article metadata.\n",
    "        keywords (list): List of keywords to check for relevance.\n",
    "        sample_size (int): Number of non-relevant articles to sample.\n",
    "\n",
    "    Returns:\n",
    "        relevant_articles (pd.DataFrame): Articles marked as relevant.\n",
    "        non_relevant_sample (pd.DataFrame): Random sample of non-relevant articles.\n",
    "        discarded_articles (pd.DataFrame): All non-relevant articles in original format.\n",
    "    \"\"\"\n",
    "    # Apply keyword detection to Title and Abstract fields\n",
    "    articles_df['Relevant'] = articles_df['Title'].apply(lambda x: contains_keywords(x, keywords)) | \\\n",
    "                              articles_df['Abstract'].apply(lambda x: contains_keywords(x, keywords))\n",
    "\n",
    "    # Separate relevant and non-relevant articles\n",
    "    relevant_articles = articles_df[articles_df['Relevant']]\n",
    "    non_relevant_articles = articles_df[~articles_df['Relevant']]\n",
    "\n",
    "    # Randomly sample non-relevant articles for manual review\n",
    "    non_relevant_sample = non_relevant_articles.sample(n=min(sample_size, len(non_relevant_articles)), random_state=42)\n",
    "\n",
    "    return relevant_articles, non_relevant_sample, non_relevant_articles\n",
    "\n",
    "def generate_relevant_authors(relevant_articles_df, authors_df, output_path):\n",
    "    \"\"\"\n",
    "    Generate the relevant authors table based on the relevant articles and authors tables.\n",
    "\n",
    "    Args:\n",
    "        relevant_articles_df (pd.DataFrame): DataFrame of relevant articles.\n",
    "        authors_df (pd.DataFrame): DataFrame of authors.\n",
    "        output_path (str): Path to save the resulting relevant authors CSV file.\n",
    "    \"\"\"\n",
    "    # Filter the authors table to include only PMIDs from the relevant articles\n",
    "    relevant_authors_df = authors_df[authors_df['PMID'].isin(relevant_articles_df['PMID'])]\n",
    "\n",
    "    # Save the resulting relevant authors table\n",
    "    relevant_authors_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Define file paths\n",
    "articles_path = 'data/articles.schistosomiasis.csv'\n",
    "authors_path = 'data/authors.schistosomiasis.csv'\n",
    "\n",
    "# Load data\n",
    "articles = pd.read_csv(articles_path)\n",
    "authors = pd.read_csv(authors_path)\n",
    "\n",
    "# Define relevant keywords\n",
    "keywords = [\"schistosomiasis\", \"schistosoma\", \"parasitic disease\", \"schistosomal\",\n",
    "    \"japonicum\", \"oncomelania\", \"cercariae\", \"molluscicide\", \"bilharzia\", \"schistosome\", \"antischistosomal\", \"schistosomes\", \"molluscicidal\", \"snail control\"]\n",
    "\n",
    "# Filter articles and sample non-relevant ones\n",
    "relevant_articles, non_relevant_sample, discarded_articles = filter_articles(articles, keywords, sample_size=5)\n",
    "\n",
    "# Save the relevant articles and discarded articles\n",
    "relevant_articles.to_csv('data/relevant_articles.csv', index=False)\n",
    "discarded_articles.to_csv('data/discarded_articles.csv', index=False)\n",
    "\n",
    "# Generate relevant authors\n",
    "output_authors_path = 'data/relevant_authors.csv'\n",
    "generate_relevant_authors(relevant_articles, authors, output_authors_path)\n",
    "\n",
    "# Print sample of non-relevant articles for manual review\n",
    "print(\"Sample of non-relevant articles for manual review:\")\n",
    "print(non_relevant_sample[['PMID', 'Abstract']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get final relevant articles table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset saved to data/relevant_articles_final.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load Datasets\n",
    "articles_path = 'data/relevant_articles.csv'\n",
    "scimagojr_path = 'data/scimagojr_2023.csv'\n",
    "\n",
    "articles_df = pd.read_csv(articles_path)\n",
    "scimagojr = pd.read_csv(scimagojr_path, sep=';')\n",
    "\n",
    "# Step 2: Clean and Standardize Columns\n",
    "# Remove spaces from ISSN and ensure consistent formatting\n",
    "articles_df['ISSN'] = articles_df['ISSN'].str.replace(' ', '').str.lower()\n",
    "scimagojr['Issn'] = scimagojr['Issn'].str.replace(' ', '').str.lower()\n",
    "\n",
    "# Standardize journal names for matching\n",
    "articles_df['Journal'] = articles_df['Journal'].str.strip().str.lower()\n",
    "scimagojr['Title'] = scimagojr['Title'].str.strip().str.lower()\n",
    "\n",
    "# Step 3: Clean SJR Values\n",
    "# Replace commas with dots and convert to numeric\n",
    "scimagojr['SJR'] = scimagojr['SJR'].astype(str).str.replace(',', '.').astype(float)\n",
    "\n",
    "# Step 4: Match and Add Impact Factor\n",
    "# Match by ISSN first\n",
    "merged = pd.merge(\n",
    "    articles_df,\n",
    "    scimagojr[['Issn', 'SJR']],\n",
    "    left_on='ISSN',\n",
    "    right_on='Issn',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing SJR using journal names as a fallback\n",
    "merged['SJR'] = merged['SJR'].fillna(\n",
    "    pd.merge(\n",
    "        articles_df,\n",
    "        scimagojr[['Title', 'SJR']],\n",
    "        left_on='Journal',\n",
    "        right_on='Title',\n",
    "        how='left'\n",
    "    )['SJR']\n",
    ")\n",
    "\n",
    "# Replace missing SJR with 0\n",
    "merged['SJR'] = merged['SJR'].fillna(0)\n",
    "\n",
    "# Step 5: Add \"Field\" Column\n",
    "# Define academic fields and associated keywords\n",
    "fields_keywords = {\n",
    "    \"Medical Sciences\": [\"infection\", \"hospitalised\", \"hospitalized\", \"circulating\", \"treatment\", \"disease\"],\n",
    "    \"Epidemiology and Public Health\": [\"epidemiology\", \"public health\", \"population\", \"spread\", \"incidence\", \"prevalence\"],\n",
    "    \"Parasitology and Tropical Medicine\": [\"schistosoma\", \"parasite\", \"parasitology\", \"tropical medicine\"],\n",
    "    \"Immunology\": [\"antigen\", \"immune\", \"immunology\", \"immune response\", \"antigenic\"],\n",
    "    \"Biological and Biomedical Research\": [\"nitric oxide\", \"biomedical\", \"biology\", \"cellular\", \"biochemistry\"]\n",
    "}\n",
    "\n",
    "# Function to classify articles based on keyword frequency\n",
    "def classify_article_by_frequency(title, abstract):\n",
    "    combined_text = f\"{title} {abstract}\".lower() if not pd.isnull(title) and not pd.isnull(abstract) else \"\"\n",
    "    field_counts = defaultdict(int)\n",
    "    \n",
    "    for field, keywords in fields_keywords.items():\n",
    "        for keyword in keywords:\n",
    "            field_counts[field] += combined_text.count(keyword.lower())\n",
    "    \n",
    "    # Return the field with the highest count, or \"Other\" if all counts are zero\n",
    "    if field_counts:\n",
    "        return max(field_counts, key=field_counts.get)\n",
    "    return \"Other\"\n",
    "\n",
    "# Apply classification to each article\n",
    "merged[\"Field\"] = merged.apply(lambda row: classify_article_by_frequency(row[\"Title\"], row[\"Abstract\"]), axis=1)\n",
    "\n",
    "# Step 6: Drop Unnecessary Columns\n",
    "merged = merged.drop(columns=['Issn'])\n",
    "\n",
    "# Step 7: Save the Consolidated Dataset\n",
    "output_path = 'data/relevant_articles_final.csv'\n",
    "merged.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Final dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get relevent authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant authors with fields, average impact factors, and article counts saved to: data/relevant_authors_with_field.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the classified articles and relevant authors\n",
    "articles_with_field_path = 'data/relevant_articles_final.csv'\n",
    "relevant_authors_path = 'data/relevant_authors.csv'\n",
    "\n",
    "articles_df = pd.read_csv(articles_with_field_path)\n",
    "authors_df = pd.read_csv(relevant_authors_path)\n",
    "\n",
    "# Ensure columns exist\n",
    "if \"PMID\" not in articles_df.columns or \"Field\" not in articles_df.columns or \"SJR\" not in articles_df.columns:\n",
    "    raise KeyError(\"The 'relevant_articles_final.csv' must contain 'PMID', 'Field', and 'SJR' columns.\")\n",
    "if \"PMID\" not in authors_df.columns or \"AuthorForename\" not in authors_df.columns or \"AuthorLastname\" not in authors_df.columns:\n",
    "    raise KeyError(\"The 'relevant_authors.csv' must contain 'PMID', 'AuthorForename', and 'AuthorLastname' columns.\")\n",
    "\n",
    "# Merge authors with article fields and SJR (impact factors) using PMID\n",
    "authors_with_field_df = pd.merge(authors_df, articles_df[['PMID', 'Field', 'SJR']], on='PMID', how='left')\n",
    "\n",
    "# Calculate the average impact factor and article count for each author\n",
    "author_stats = (\n",
    "    authors_with_field_df.groupby(['AuthorForename', 'AuthorLastname'])\n",
    "    .agg(\n",
    "        AverageImpactFactor=('SJR', 'mean'),\n",
    "        ArticleCount=('PMID', 'count')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge the calculated statistics back into the authors DataFrame\n",
    "authors_with_field_df = pd.merge(\n",
    "    authors_with_field_df, \n",
    "    author_stats, \n",
    "    on=['AuthorForename', 'AuthorLastname'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Save the resulting DataFrame to a new CSV file\n",
    "output_path = 'data/relevant_authors_with_field.csv'\n",
    "authors_with_field_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Inform the user\n",
    "print(f\"Relevant authors with fields, average impact factors, and article counts saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
